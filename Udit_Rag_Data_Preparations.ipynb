{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxxl2gjgWypPBbAZN5aFy4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uditsingh7/AI-Collab/blob/main/Udit_Rag_Data_Preparations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hmjlkGuGH_-G"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#for PDF file we need to import PyPDFLoader from langchain framework\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnUdvW0wIMXh",
        "outputId": "bdf2b54f-aaec-4f1f-822a-ea3d1db992d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.60)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion, adding the files to the collab session for this case"
      ],
      "metadata": {
        "id": "5a7_MCiqJ5ZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "X9o-AvOUIdDa",
        "outputId": "6ab37191-597f-4b97-b861-6430be251d61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d1b7ee8-312f-4d18-b49c-2035ec77f014\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7d1b7ee8-312f-4d18-b49c-2035ec77f014\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Attention_is_all_you_need.pdf to Attention_is_all_you_need.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning: After ingestion, loading, cleaning it and splitting it"
      ],
      "metadata": {
        "id": "ysWlq6V7JeAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for CSV file we need to import csv_loader\n",
        "# for Doc we need to import UnstructuredWordDocumentLoader\n",
        "# for Text document we need to import TextLoader\n",
        "\n",
        "filePath = \"Attention_is_all_you_need.pdf\"\n",
        "loader = PyPDFLoader(filePath)\n",
        "#Load document\n",
        "pages = loader.load_and_split()\n",
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKB_6UNOIXhd",
        "outputId": "261a3b51-7a40-4255-b92f-45a90aa05a26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provided proper attribution is provided, Google hereby grants permission to\n",
            "reproduce the tables and figures in this paper solely for use in journalistic or\n",
            "scholarly works.\n",
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗ †\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗ ‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "be superior in quality while being more parallelizable and requiring significantly\n",
            "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
            "to-German translation task, improving over the existing best results, including\n",
            "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
            "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
            "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
            "best models from the literature. We show that the Transformer generalizes well to\n",
            "other tasks by applying it successfully to English constituency parsing both with\n",
            "large and limited training data.\n",
            "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
            "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
            "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
            "attention and the parameter-free position representation and became the other person involved in nearly every\n",
            "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
            "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
            "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
            "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
            "our research.\n",
            "†Work performed while at Google Brain.\n",
            "‡Work performed while at Google Research.\n",
            "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
            "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking"
      ],
      "metadata": {
        "id": "DIA3KdOdKPta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[5].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZQaWuiTI9qr",
        "outputId": "d87e2947-b8a9-4ca4-a1a3-5f6cdca44843"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "MultiHead(Q, K, V) = Concat(head1, ...,headh)WO\n",
            "where headi = Attention(QWQ\n",
            "i , KWK\n",
            "i , V WV\n",
            "i )\n",
            "Where the projections are parameter matricesWQ\n",
            "i ∈ Rdmodel×dk , WK\n",
            "i ∈ Rdmodel×dk , WV\n",
            "i ∈ Rdmodel×dv\n",
            "and WO ∈ Rhdv×dmodel .\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
            "and the memory keys and values come from the output of the encoder. This allows every\n",
            "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
            "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
            "[38, 2, 9].\n",
            "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
            "and queries come from the same place, in this case, the output of the previous layer in the\n",
            "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
            "encoder.\n",
            "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
            "all positions in the decoder up to and including that position. We need to prevent leftward\n",
            "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
            "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
            "of the softmax which correspond to illegal connections. See Figure 2.\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
            "connected feed-forward network, which is applied to each position separately and identically. This\n",
            "consists of two linear transformations with a ReLU activation in between.\n",
            "FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\n",
            "While the linear transformations are the same across different positions, they use different parameters\n",
            "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
            "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
            "dff = 2048.\n",
            "3.4 Embeddings and Softmax\n",
            "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
            "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
            "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
            "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
            "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
        "all positions in the decoder up to and including that position. We need to prevent leftward\n",
        "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
        "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
        "of the softmax which correspond to illegal connections. See Figure 2.\n",
        "3.3 Position-wise Feed-Forward Networks\n",
        "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
        "connected feed-forward network, which is applied to each position separately and identically. This\n",
        "consists of two linear transformations with a ReLU activation in between.\n",
        "FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\n",
        "While the linear transformations are the same across different positions, they use different parameters\n",
        "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
        "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
        "dff = 2048.\n",
        "3.4 Embeddings and Softmax\n",
        "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
        "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
        "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
        "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
        "linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel\"\"\" # your text\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 128,\n",
        "    chunk_overlap  = 20\n",
        ")\n",
        "docs = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "GdfDa9FaJvbT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_m1ZtcfKQc6",
        "outputId": "e87e887f-50e8-4bc3-af46-54ecda2bd203"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Similarly, self-attention layers in the decoder allow each position in the decoder to attend to'),\n",
              " Document(metadata={}, page_content='all positions in the decoder up to and including that position. We need to prevent leftward'),\n",
              " Document(metadata={}, page_content='information flow in the decoder to preserve the auto-regressive property. We implement this'),\n",
              " Document(metadata={}, page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input'),\n",
              " Document(metadata={}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks'),\n",
              " Document(metadata={}, page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully'),\n",
              " Document(metadata={}, page_content='connected feed-forward network, which is applied to each position separately and identically. This'),\n",
              " Document(metadata={}, page_content='consists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)'),\n",
              " Document(metadata={}, page_content='While the linear transformations are the same across different positions, they use different parameters'),\n",
              " Document(metadata={}, page_content='from layer to layer. Another way of describing this is as two convolutions with kernel size 1.'),\n",
              " Document(metadata={}, page_content='The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.'),\n",
              " Document(metadata={}, page_content='dff = 2048.\\n3.4 Embeddings and Softmax'),\n",
              " Document(metadata={}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input'),\n",
              " Document(metadata={}, page_content='tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-'),\n",
              " Document(metadata={}, page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In'),\n",
              " Document(metadata={}, page_content='our model, we share the same weight matrix between the two embedding layers and the pre-softmax'),\n",
              " Document(metadata={}, page_content='linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "from langchain.text_splitter import NLTKTextSplitter"
      ],
      "metadata": {
        "id": "L4ARoN1YKRrw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WhtZt4WZdeR",
        "outputId": "4ed9a5cd-5c87-42b9-8b6d-f500ab05ccb8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.60)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.42)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"LangChain is a powerful framework. It helps in building AI applications. What do you think?\"\n",
        "\n",
        "text_splitter = NLTKTextSplitter()\n",
        "docs = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "-pcT3bR5QJ2e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvL074wzWPnc",
        "outputId": "a3215451-d866-43d5-e1f1-0fbe30ebbe31"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LangChain is a powerful framework.\\n\\nIt helps in building AI applications.\\n\\nWhat do you think?']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS2NOE5yWcr1",
        "outputId": "aee31506-0042-4114-e5b1-6635e2cb4918"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LangChain is a powerful framework.\\n\\nIt helps in building AI applications.\\n\\nWhat do you think?']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import SpacyTextSplitter\n"
      ],
      "metadata": {
        "id": "zowGr3UVWf0j"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SpacyTextSplitter()\n",
        "docs = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "LJcJbpw4Ywdk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UuOx842Znqt",
        "outputId": "5378c425-792b-4672-88db-b77b493231ab"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position.\\n\\nWe need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property.\\n\\nWe implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections.\\n\\nSee Figure 2.\\n\\n\\n3.3 Position-wise Feed-Forward Networks\\n\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically.\\n\\nThis\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\n\\n\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer.\\n\\nAnother way of describing this is as two convolutions with kernel size 1.\\n\\n\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n\\n\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel.\\n\\nWe also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities.\\n\\nIn\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30].\\n\\nIn the embedding layers, we multiply those weights by √dmodel']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: DEFINE YOUR INPUT ---\n",
        "# We will define the input as a simple string, which is what split_text expects.\n",
        "my_input_data = \"LangChain is a powerful framework. It helps in building AI applications. What do you think?\"\n",
        "\n",
        "print(\"--- 1. INSPECTING THE INPUT ---\")\n",
        "print(f\"My input data is: '{my_input_data}'\")\n",
        "print(f\"The TYPE of my input is: {type(my_input_data)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- STEP 2: DEFINE THE SPLITTER ---\n",
        "# We are explicitly creating the NLTK splitter right here.\n",
        "my_splitter = NLTKTextSplitter()\n",
        "\n",
        "print(\"--- 2. INSPECTING THE SPLITTER ---\")\n",
        "print(f\"The splitter object is: {my_splitter}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- STEP 3: PERFORM THE SPLIT ---\n",
        "# We use the variables defined right here in this cell.\n",
        "final_chunks = my_splitter.split_text(my_input_data)\n",
        "\n",
        "print(\"--- 3. INSPECTING THE OUTPUT ---\")\n",
        "print(f\"The final chunks are: {final_chunks}\")\n",
        "print(f\"The TYPE of the output is: {type(final_chunks)}\")\n",
        "print(f\"Number of chunks created: {len(final_chunks)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- STEP 4: FINAL CHECK ---\n",
        "if len(final_chunks) > 1:\n",
        "    print(\"\\n✅ SUCCESS: The text was split into multiple chunks.\")\n",
        "else:\n",
        "    print(\"\\n❌ FAILURE: The text was not split. One of the steps above is wrong in your original code.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vuXSQwnYy04",
        "outputId": "a9046000-6634-403b-dae9-24635a55dea1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. INSPECTING THE INPUT ---\n",
            "My input data is: 'LangChain is a powerful framework. It helps in building AI applications. What do you think?'\n",
            "The TYPE of my input is: <class 'str'>\n",
            "------------------------------\n",
            "--- 2. INSPECTING THE SPLITTER ---\n",
            "The splitter object is: <langchain_text_splitters.nltk.NLTKTextSplitter object at 0x7f93860d8350>\n",
            "------------------------------\n",
            "--- 3. INSPECTING THE OUTPUT ---\n",
            "The final chunks are: ['LangChain is a powerful framework.\\n\\nIt helps in building AI applications.\\n\\nWhat do you think?']\n",
            "The TYPE of the output is: <class 'list'>\n",
            "Number of chunks created: 1\n",
            "------------------------------\n",
            "\n",
            "❌ FAILURE: The text was not split. One of the steps above is wrong in your original code.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 256,\n",
        "    chunk_overlap  = 20\n",
        ")"
      ],
      "metadata": {
        "id": "V8PNlewQZFZq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "SjRouUk0Z6qG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn50mKHlaAwM",
        "outputId": "e9ab89e8-b68b-4071-b0ae-bc603af20100"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward',\n",
              " 'information flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.',\n",
              " '3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This',\n",
              " 'consists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters',\n",
              " 'from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax',\n",
              " 'Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-',\n",
              " 'mation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax',\n",
              " 'linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Markdown chnking\n",
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "markdown_text = text"
      ],
      "metadata": {
        "id": "foazCPKmaBMY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "docs = markdown_splitter.create_documents([markdown_text])\n"
      ],
      "metadata": {
        "id": "W6UbszwXaYZa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_nfL_OXacX8",
        "outputId": "715642f5-698a-45a3-f941-05b533327390"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='Similarly, self-attention layers in the decoder allow each position in the decoder to attend to'),\n",
              " Document(metadata={}, page_content='all positions in the decoder up to and including that position. We need to prevent leftward'),\n",
              " Document(metadata={}, page_content='information flow in the decoder to preserve the auto-regressive property. We implement this'),\n",
              " Document(metadata={}, page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input'),\n",
              " Document(metadata={}, page_content='of the softmax which correspond to illegal connections. See Figure 2.'),\n",
              " Document(metadata={}, page_content='3.3 Position-wise Feed-Forward Networks'),\n",
              " Document(metadata={}, page_content='In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully'),\n",
              " Document(metadata={}, page_content='connected feed-forward network, which is applied to each position separately and identically. This'),\n",
              " Document(metadata={}, page_content='consists of two linear transformations with a ReLU activation in between.'),\n",
              " Document(metadata={}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)'),\n",
              " Document(metadata={}, page_content='While the linear transformations are the same across different positions, they use different'),\n",
              " Document(metadata={}, page_content='parameters'),\n",
              " Document(metadata={}, page_content='from layer to layer. Another way of describing this is as two convolutions with kernel size 1.'),\n",
              " Document(metadata={}, page_content='The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality'),\n",
              " Document(metadata={}, page_content='dff = 2048.\\n3.4 Embeddings and Softmax'),\n",
              " Document(metadata={}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input'),\n",
              " Document(metadata={}, page_content='tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear'),\n",
              " Document(metadata={}, page_content='transfor-'),\n",
              " Document(metadata={}, page_content='mation and softmax function to convert the decoder output to predicted next-token probabilities. In'),\n",
              " Document(metadata={}, page_content='our model, we share the same weight matrix between the two embedding layers and the pre-softmax'),\n",
              " Document(metadata={}, page_content='linear transformation, similar to [30]. In the embedding layers, we multiply those weights by'),\n",
              " Document(metadata={}, page_content='√dmodel')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latex_text = \"\"\"\n",
        "\\\\documentclass{article}\n",
        "\\\\usepackage{amsmath}\n",
        "\\\\title{A Study on Advanced Document Chunking}\n",
        "\\\\author{Dr. Gemini}\n",
        "\\\\date{\\\\today}\n",
        "\n",
        "\\\\begin{document}\n",
        "\n",
        "\\\\maketitle\n",
        "\n",
        "\\\\begin{abstract}\n",
        "This paper explores the theoretical underpinnings of document chunking for Retrieval-Augmented Generation (RAG). We focus on structured documents, particularly those written in LaTeX, to preserve semantic context.\n",
        "\\\\end{abstract}\n",
        "\n",
        "\\\\section{Introduction}\n",
        "Document analysis is a critical first step in any RAG pipeline. The quality of text chunks directly impacts retrieval accuracy. Traditional methods often rely on fixed-size splitting, which can break apart key concepts. A more robust approach considers the document's inherent structure. For instance, the famous mass-energy equivalence is described by the inline equation $E=mc^2$.\n",
        "\n",
        "\\\\subsection{Core Principles}\n",
        "There are several core principles for intelligent chunking. A good system should be aware of the following:\n",
        "\\\\begin{itemize}\n",
        "    \\\\item Semantic boundaries: Grouping complete thoughts or ideas.\n",
        "    \\\\item Structural markers: Using document headers and environments as delimiters.\n",
        "    \\\\item Metadata preservation: Storing the original location (e.g., section title) with each chunk.\n",
        "\\\\end{itemize}\n",
        "These principles ensure that retrieved contexts are both complete and relevant.\n",
        "\n",
        "\\\\section{Methodology}\n",
        "Our methodology leverages a structural parsing approach. We treat the LaTeX source as a tree and create chunks based on its nodes. This is particularly effective for mathematical content. For example, a key equation in machine learning is the mean squared error loss function.\n",
        "\\\\begin{equation}\n",
        "\\\\mathcal{L}(x, y) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i - f(x_i))^2\n",
        "\\\\label{eq:loss}\n",
        "\\\\end{equation}\n",
        "As shown in Equation \\\\ref{eq:loss}, this captures the average squared difference between estimated values and actual values. A simple splitter might incorrectly break this equation.\n",
        "\n",
        "\\\\end{document}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "QL6SCtWOahBh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Latex splitter\n",
        "from langchain.text_splitter import LatexTextSplitter\n",
        "latex_splitter = LatexTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "docs = latex_splitter.create_documents([latex_text])"
      ],
      "metadata": {
        "id": "jj_GCeW4a92n"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhCjSKCZbXNU",
        "outputId": "11529cac-1609-4a48-93b9-fe743a8ca546"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='\\\\documentclass{article}\\n\\\\usepackage{amsmath}\\n\\\\title{A Study on Advanced Document'),\n",
              " Document(metadata={}, page_content='Chunking}\\n\\\\author{Dr. Gemini}\\n\\\\date{\\\\today}\\n\\n\\\\begin{document}\\n\\n\\\\maketitle\\n\\n\\\\begin{abstract}\\nThis'),\n",
              " Document(metadata={}, page_content='paper explores the theoretical underpinnings of document chunking for Retrieval-Augmented'),\n",
              " Document(metadata={}, page_content='Generation (RAG). We focus on structured documents, particularly those written in LaTeX, to'),\n",
              " Document(metadata={}, page_content='preserve semantic context.\\n\\\\end{abstract}\\n\\n\\\\section{Introduction}\\nDocument analysis is a critical'),\n",
              " Document(metadata={}, page_content='first step in any RAG pipeline. The quality of text chunks directly impacts retrieval accuracy.'),\n",
              " Document(metadata={}, page_content='Traditional methods often rely on fixed-size splitting, which can break apart key concepts. A more'),\n",
              " Document(metadata={}, page_content=\"robust approach considers the document's inherent structure. For instance, the famous mass-energy\"),\n",
              " Document(metadata={}, page_content='equivalence is described by the inline equation'),\n",
              " Document(metadata={}, page_content='$E=mc^2'),\n",
              " Document(metadata={}, page_content='$.\\n\\n\\\\subsection{Core Principles}\\nThere are several core principles for intelligent chunking. A good'),\n",
              " Document(metadata={}, page_content='system should be aware of the following:\\n\\\\begin{itemize}\\n    \\\\item Semantic boundaries: Grouping'),\n",
              " Document(metadata={}, page_content='complete thoughts or ideas.\\n    \\\\item Structural markers: Using document headers and environments'),\n",
              " Document(metadata={}, page_content='as delimiters.\\n    \\\\item Metadata preservation: Storing the original location (e.g., section title)'),\n",
              " Document(metadata={}, page_content='with each chunk.\\n\\\\end{itemize}\\nThese principles ensure that retrieved contexts are both complete'),\n",
              " Document(metadata={}, page_content='and relevant.\\n\\n\\\\section{Methodology}\\nOur methodology leverages a structural parsing approach. We'),\n",
              " Document(metadata={}, page_content='treat the LaTeX source as a tree and create chunks based on its nodes. This is particularly'),\n",
              " Document(metadata={}, page_content='effective for mathematical content. For example, a key equation in machine learning is the mean'),\n",
              " Document(metadata={}, page_content='squared error loss function.\\n\\\\begin{equation}\\n\\\\mathcal{L}(x, y) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} (y_i -'),\n",
              " Document(metadata={}, page_content='f(x_i))^2\\n\\\\label{eq:loss}\\n\\\\end{equation}\\nAs shown in Equation \\\\ref{eq:loss}, this captures the'),\n",
              " Document(metadata={}, page_content='average squared difference between estimated values and actual values. A simple splitter might'),\n",
              " Document(metadata={}, page_content='incorrectly break this equation.\\n\\n\\\\end{document}')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Tokensization"
      ],
      "metadata": {
        "id": "v1pCT4BzdHaf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TonuxqQ5bXtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}